{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Supervised Learning - k-Nearest Neighbor (kNN) Algorithm \n",
    "\n",
    "In this lab, we will make first steps in doint supervised learning. in particular, we will learn about the k-Nearest Neighbor (kNN) algorithm. kNN uses a simple idea: \"you are what your neighbors are\". This idea work quite well in data science. In the first part of the lab, we will cover some background needed to understand the kNN algorithm. In the second part, you will be asked to apply your knowledge on another data set. \n",
    "\n",
    "## Lab 7.A: kNN Tutorial with Questions (50% of grade)\n",
    "\n",
    "As usual, let us start by importing the needed libraries. We will continue using the sklearn library, which implements many of the most popular data science algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the Iris data set using a sklearn function `load_iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html, `iris` is an object with attributes `data` (a 150x4 matrix, where $i$-th row are 4 attributes of the $i$-th flower), `feature_names` (the names of the 4 attributes -- remember that in data science \"attribute\" and \"feature\" means the same thing), `target` (a vector of length 150, where $i$-th number is the type of the $i$-th flower -- in data science people often say \"label\" instead of \"target\"), `target_names` (these are strings explaining what each of the 3 types of flowers are), and `DESCR` (giving some information about the Iris data set). Let us list them all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print iris.DESCR\n",
    "print iris.data\n",
    "print iris.feature_names\n",
    "print iris.target\n",
    "print iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the attributes of the second flower are `[4.9, 3.0, 1.4, 0.2]`, which means its `sepal_length` is 4.9 cm, `sepal_width` is 3.0 cm, `petal_length` is 1.4 cm, and `petal_width` is 0.2 cm. We will write it matematically as $x_2 = [x_{21}, x_{22}, x_{23}, x_{24}] = [4.9, 3.0, 1.4, 0.2]$. We see that its `target` is 0, which means the type of this iris is `setosa`. We will write it matematically as $y_2 = 0$. All this information was obtained by real botanists who studied iris flowers trying to understand the physical measurements that discriminate between the 3 different types of those flowers.\n",
    "\n",
    "In data science, people like to denote this data set as $D_{Iris} = \\{(x_i, y_i), i = 1, 2 ... 150\\}$, meaning that data set $D_{Iris}$ is a set of 150 labeled examples $(x_i, y_i)$. An alternative is to write $D_{Iris} = \\{X_{Iris}, Y_{Iris}\\}$.\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning is a game with the following objective. You are given the iris data set $D_{Iris}$ where you know 4 attributes and target values for 150 irises and your objective is to come up with a computer program that predicts a type of any iris flower given the values of its 4 attributes. Written in pseudocode, this is what you have to do:\n",
    "\n",
    "`predictor = create(algorithm_type, D)\n",
    "y_new = predictor(x_new)`\n",
    "\n",
    "In the first line, you are running a `create` function that takes as input data set `D` and the name of a supervised learning algorithm `algorithm_type` and produces as an output a computer program `predictor`. In the second line, you are using `predictor` to predict the label (`y_new` value) for a flower whose attributes are given by `x_new`.\n",
    "\n",
    "### kNN Algorithm\n",
    "kNN is a popular supervised learning algorithm that allows us to create `predictor`. The idea of kNN is that the label of flower `x_new` depends on labels of flowers in its neighborhood. In particular, kNN finds the distance between `x_new` and every example `x` in data set `D`. Then, it looks at the label `y` of k examples which are the closest to `x_new`. The predicted label `y_new` is obtained as the most common label in the group of the k nearest neighbors.\n",
    "\n",
    "**Parameter choice**. We need to make a few decisions when running kNN. The most important is the choice of `k`. If `k = 1`, then we are looking only at the hearest neighbor and it might not be a good idea if we are dealing with noisy data. If `k` is very large, then we might be counting far neighbors that might have different properties. Other decisions include the choice of distance metric (Euclidean is the standard one) and the choice whether to weight closer neighbors more than the farther ones.\n",
    "\n",
    "**Accuracy**. When deciding which parameters to pick or which supervised learning algorithm to use (there are popular algorithms other than kNN), the question is how to measure which choice is better. The answer is to check if `predictor` provides accurate prediction. Given a data set `D`, a typical way to check accuracy is to randomly split `D` into two data sets, `D_train` and `D_test`. Then, `predictor` is created/trained using `D_train` data set and its accuracy is checked using `D_test`. In particular, we use `predictor` to predict label of every example from `D_test` and compare it with the true labels. The percentage of the correct guesses on `D_test` is reported as accuracy of `predictor`.\n",
    "\n",
    "## kNN Demo\n",
    "The following piece of code is taken from:\n",
    "http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py. Let us run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15039250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADSCAYAAABq3So1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXl8lNXV+L9nMpPJRsK+7yjIooKiVmVRqVatWq2AomDVWivWvtVaW/WnVVv31q2tVlHrBmJBpb4q1OWlIIhowZV9E8Iq+5J9ktzfH/cJmcw8k8wks2Q5Xz7zIblzc++Z+9znzHnOPfdcMcagKIqiNB08qRZAURRFiQ1V3IqiKE0MVdyKoihNDFXciqIoTQxV3IqiKE0MVdyKoihNjGaruEXkbhGZ2tzlEJHlInKa87OIyAsisk9EPhORkSKyOgF99hSRAhFJi3fbTvvTReRC5+crRWRhIvppKojI0yJyZ5R1XxSRexMtU7wIljdR8zUZiMj/iMiDyeqv0ShuEZkqIttF5KCIrBGRa6L4m8tEZImjRLaLyBwRGZEMeRsLxpjBxph5zq8jgDOB7saYE40xC4wxAxrah4hsFJHvB/WZb4zJMcZUNLRtl76OAY4F3op327X0eYuILBORQyLyrYjcEvJ+bxH5j4gUiciq4LFIBsaY64wxf4xHWyJiROSIeLQVZX+9RWRjNHWjna+NyCibV2U0AVOAiSLSMRl9NxrFDTwA9DbG5AIXAPeKyPGRKovIr4HHgfuBTkBP4CngR0mQtbHSC9hojClMtSAN4OfANJPcnWECXAG0Ac4GbhCRS4Penw58AbQD/h/wuoh0SKJ8SiPHGFMCzMHOo4TTaBS3MWa5Maa06lfn1c+trojkAX8AfmGMedMYU2iMCRhj3jbG3BLhb2aKyA4ROSAiH4nI4KD3zhWRFY7FtVVEfuOUtxeRd0Rkv4jsFZEFIuI6ZiIyWEQ+cOp9JyK3J0OOKmtYRH4KPAec7DyB3CMip4nIlqD2e4jImyKyS0T2iMjfnPJ+IjLXKdstItNEpLXz3ivYL8W3nXZ/61hRRkS8Tp2uIvK/jmzrRORnQX3eLSIzRORl53MtF5HhbmPjcA4wP9KbIvInEVnozIG4YIx52BjzuTGm3BizGmvtn+r01x84DrjLGFNsjHkD+Aa4uK52ReQqEXk76Pd1IjIj6PfNIjLU+fmooPmzWkTGB9Wr4f5wrsF2EdkmIte4WNFtRORdZ7w/FZF+zt995Lz/lXMtL4lljscDERkmIp87sv0TyAh6L3S+/s65Dw45YzJGRM4GbgcucT7DV07dq0RkpVN3g4j8PLRdEblZRHY6Y3dV0PuZIvKIiGxy7suFIpLpvPc9EVnkjM9XUm1huzEP+GGchqp2jDGN5oW1mIuwSvtzICdCvbOBcsBbS1t3A1ODfr8aaAX4sZb6l0HvbQdGOj+3AY5zfn4AeBrwOa+RgLj01cpp42bsRGwFnJQMOYCNwPedn68EFga1dxqwxfk5DfgKeAzIduQc4bx3BNbF4gc6AB8Bjwe1c7gP5/fezjXyOr/Pd65dBjAU2AWMCfr8JcC5jgwPAIsjXLNsp90OQWVXAguxRsazwHtAVoS/vwzYX8urZxRzULDW9XXO7xcBK0Pq/A34axRt9XX69QBdgE3A1qD39jnvZQObgasAL/aLYjcw2Kn7InBv0NzfAQwGsoBXnDE7IqjuXuBEp61pwGtBMh2uG8scd+p+XcvYPhXFeKQ7Y3CT09dYIBD02U6jer4OcMaka9Cc6+d2TzllP8QaegKMxuqR44LaLccaez7sXCwC2jjvP4lVut2wc/QU7L3QDdjj1Pdg75E9BM3PEBmOA/YmQjeGvhqNxQ1gjLkeq9RGAm8CpRGqtgN2G2PKY2j7H8aYQ8Za9XcDxwZZbQFgkIjkGmP2GWM+DyrvAvQy1qJfYJwrFMJ5wA5jzCPGmBKnn09TIEdtnAh0BW4x9gmlxBiz0JFpnTHmA2NMqTFmF/AodvLXiYj0wPrWf+e0+SXW8p8UVG2hMWa2sT7xV7A+bDdaO/8fCin3Yd0VbYHzjTFFbn9sjHnVGNO6lld+FB/pbuxN+oLzew5wIKTOAew8rRVjzAbnswzFjud7wFYROcr5fYExphI7fzYaY14w1ur/HHgDq9hCGQ+8YOwTahFwj0udN40xnzn3xzSn/0hEPbeMMcfUMrbX1zUewPew1/Jxp6/Xgf9GqFuBVZ6DRMRnjNlojFkfqWFjzLvGmPXGMh94H6tHgj/nH5x+ZwMFwADn6eJq4FfGmK3GmApjzCLn/pwIzHbmbqUx5gNgCVaRu3EIiNuTYG00KsUN4AzcQqA7MBlA7KJjgfO6HPut177qUb0uRCRNRB4UkfUichBrQQK0d/6/GHsxNonIfBE52Sn/E7AOeN95/Lo1Qhc9gIiTKoly1EYPYJPbl52IdBSR15zH0oPA1CCZ6qIr1soIVrabsNZKFTuCfi4CMiJcu/3O/6FK8Qjs2sU9xpiyKOWKGRG5Aeuj/KGpdtsVALkhVXMJ/3KJxHysxTfK+XkeVmmPptol1As4yXkc3y8i+4HLgc4u7XXFWqJVbHapEzreObXIF4+5FS1dsU8cwV8Mm9wqGmPWATdiv0h3OvOza6SGReQcEVnsuHv2Y++j4Dm8J2TuV41Le+yTotv92wsYF3JdRmC/6NxoRfiXfEJodIo7CC+Oj9sYc46xUQw5xphpwCfYx+8Lo2zrMuyN/33sN2Jvp1yc9v9rjPkR0BH4FzDDKT9kjLnZGNMXOB/4tYiMcWl/MxH88UmWozY2Az0jKMwHsI/Qxxi7ODyxSiaH2qz7bUBbEQlWtj2BrTHKh7GLquuB/iFvrcS6EeaISMSoAxG5POgL3u3Vs5a/vRq4Fevi2RL01nKgb8jnO9Ypj4YqxT3S+Xk+4Yp7MzA/xILNMcZMdmlvO9aoqaJHlHK4EsvcErs+EWlsn46iu+1ANxEJnlsRr4nzBDUCq0AN8FDVWyFy+bFPKH8GOhljWgOzqTmHI7Ebq0vc7t/NwCsh1yXbGBMp7G8g1h2ZcBqF4nYsvktFJMexSn8ATADmutU3xhwAfg88KSIXikiWiPicb92HXf6kFdbtsgfrF7w/qO9054bPM8YEgIPYxzRE5DwROcKZaFXlbiFw7wCdReRGEfGLSCsROSkFctTGZ9gb50ERyRaRDBE5NUiuAmC/iHQDQhd4v8P6ZMMwxmwGFgEPOG0eA/wU+4heH2bj4qYxxkzHLkp9KM5im0udaUFf8G4vV1eJ8xR3P3Cm494IbnMN8CVwl/P5LgKOwSqKqoWv2r7Y5gOnA5nOF8ICrJ+6HdaXDnb+9BeRSc489onICSIy0KW9GcBVIjJQRLKw90Es1LiWscwtY0NPI43tdVH0/QnW1/w/IuIVkR9jXXhhiMgAETnDUcolQHGQXN8BvaV6ETUd61bZBZSLyDnAWVHIg+Oq+gfwqNhF9jQROdnpdypwvoj8wCnPcK539wjNjcZGliScRqG4sd+gk4Et2AWbPwM3GmMixvIaYx4Ffg3cgb1gm4EbsJZqKC/jLAwBK4DFIe9PAjY6boLrsBYnwJHAh1il9gl2AWaeiyyHsAsX52MfU9dib9akylEbxvqXz8e6HfKxY32J8/Y92IWVA8C72PWFYB4A7nAeF3/j0vwE7NPDNmAWNgLjg1jkC2IKcHmIVVb1GV7CLjDNFZHe9WzfjXuxivS/ESzIS4Hh2Ln5IDDW2LUAsBbvJ5EadhR/AVZhY4w5CGwAPnauSdX8OcvpZxt2Dj2EVUah7c0B/gL8B+viqOo70npQKHcDLznXcjxxmFvR4ri5foxdcN6HnX+hc60KP3asd2PHoyP2ixtgpvP/HhH53Bm//8F+qe3DPtn+bwyi/QYbKfRf7MLuQ4DHMUp+5PRbpWNuwUVvikgG1j3zUgz91puqyARFaTSIyKvADGOM25dwo0JEngNmGmPeS1H/A4FlgN9t/UJJDiLyS6CHMea3SelPFbeiNC0cd8272DDCl4BKY0y06z1KM6CxuEoURYmen2Mf3ddj/b5ui5hKM0YtbkVRlCaGWtyKoihNDFXciqIoTYyodh7GSvvcXNO7gyZPU5S62EebVIugNBI2bFi62xgTleJMiOLu3aEDSx5MWk5xRWmyzGRcqkVQGgnjx4vr9n831FWiKClClbZSX1RxK4qiNDFUcStKClBrW2kICfFxK4rijipsJR6oxa0oitLEUMWtKIrSxFDFrShJQt0kSrxQxa0oitLEiPbMxo3YM/YqgHJjzPBECqUozQ21tpV4EktUyenGmN0Jk0RRFEWJCnWVKEqCUWtbiTfRKm4DvC8iS0Xk2kQKpCiKotROtK6SU40x20SkI/CBiKwyxnwUXMFR6NcC9GzfPs5iKkrTRK1tJRFEZXEbY7Y5/+/EnuJ9okudKcaY4caY4R1yc+MrpaI0UcYdPpBcUeJHnYpbRLJFpFXVz8BZ2FOlFUVRlBQQjcXdCVgoIl8BnwHvGmP+nVixFKX5oFa3Em/q9HEbYzYAxyZBFkVRFCUKNBxQURSliaGKW1EUpYmhiltRFKWJoYpbURKMxnIr8UYVt6IoShNDFbeiJBC1tpVEoIpbURSliaGHBStKAlBLW0kkanEriqI0MVRxK0qcUWtbSTSquBVFUZoYqrgVRVGaGKq4FUVRmhiquJXGw9698OWXsH17qiWpN+rfVpKBhgMqqaeyEp5+Gj7+GHw+KC+HAQPgllsgIyPV0ilKo0MtbiX1vP02LFoEgQAUFUFZGaxaBc89l2rJYkKtbSVZqOJWUs+cOVZZBxMIwCefWOu7CaBKW0kmqriV1FNc7F5eWRmu0BVFUR+30gg4+mj473/BmJrlnTtDVlZqZIoStbSVVKAWt5J6Jk60Ctrr2BEeD/j9cO21qZWrDlRpK6lCLW4l9XTuDI8+an3da9ZA9+5w7rnQtWuqJVOURokqbiUxbNwIM2bAt99Cp04wdiwMGRK5fps2cNllSROvoai1raQSdZUo8WfDBrjzTli6FPbsgRUr4MEHYfHiVEsWF1RpK6lGFbcSf155BUpLay42lpXBiy+GL0AqihIzqriVcMrLraVc31C89evdyw8ciBz610RQa1tpDKiPW6nJ7NnWN1218eWss2zUhyeG7/jWrWHHjvByr9dGiyiK0iDU4laqWbAApk+v3nZeVgYffACvvRZbOz/+cbiCTk+3XwJpafGTN4nMZJxa20qjQRW3Us0bb1jfdDClpfDvf0NFRfTtjB4NF19sE0RlZNjEUaed1qSiRoJRha00NtRVolSzd697eSAAJSWQnR1dOyJw4YU2FnvvXus6aUiWvw0bbHhhp04waJBtX1FaMFErbhFJA5YAW40x5yVOJCVl9OkDK1eGl+fm1m/reXq63VxTX8rKbBjh2rX2dxFo1w7uucfKpCgtlFhcJb8CXO5qpdkwcaJVtsGkp8OkSamxct94A1avtu6a0lJr9e/YAX//e/JlUZRGRFSKW0S6Az8EmlaCZCU2jjzSWrPHHGMt2iOPhN/8BkaMSI08c+daN00wFRX2lJwkZQ1U/7bSGInWVfI48FugVaQKInItcC1Az/btGy6Zkhr69YM77oi+/tat9sCD/Hxo3x6uvBIGDoyPLKFKO5jKyvj0oShNkDotbhE5D9hpjFlaWz1jzBRjzHBjzPAO6n9sGaxcCTfdBMuXw6FDNi/JXXfB//1ffNo/4QT38MFevRJ+pJmG/ymNmWhcJacCF4jIRuA14AwRmZpQqZSmwRNPuJc//3x82r/8csjLq44JT0+3i6TXXx+f9l1Qha00Bep0lRhjbgNuAxCR04DfGGMmJlgupSkQKXywvNy+17Ztw9pv3Roef9xuDFq7Frp1g9NPT1hEiSpspamgcdwtlWeeqenSOPpom9EvFkQiJ42K1ZXxzTc2OdXWrTbkb/x4uyiakQFnnmlfCUSVdmKJdHnjVb+lEdPOSWPMPI3hbgY8+2y4H/qbb+C222JrJ1J+7XbtYov7XrYMHnrIbrIJBGzI3zPP2O32SUCVdmKJ9fKmeDo0CXTLe0vkww/dy9evjy3M7ne/s0o6mIwM+OMfY5Pn1VfD+y0ttTlSEhw9oko78cR6eVM4HZoM6ippidSWE3v7dhu14UZBgX2/fXt7Yk16ut0Ms2SJPSRh4EAYMyZ2ebZscS8vLravaLfaR4kq6+QS6+VN8nRokqjiVmrSpUt4mTHW4fjeezZhVCAAxx8Pv/gF/POf1eWffAJffQU33BC+A7M2OnSAzZvDy9PTITOz/p/FBVXaySfWy5vE6dBkUVdJSySSRZ2R4a5w33vPOhgDAZvyNRCAzz+He+91L3/xxdjkufTS8H79frjggtjygCuNklgvr06HutFhaIkUFbmXBwLVBygE8/bb4eley8qq84iEls+f795OJE44AX7+c+t+8Xjss/DYsXDRRdG3UQcan506Yr28SZgOTR51laSaigprpW7aBB07wve+F5uboT7tFxRErl9WZk+qCaa2+m5UVrq3UxsjR9p4r0DAul3ilNRKlXVNEj3dIhHr5U3QdGg2qOJOJYWFNnZ6926b+S4jw/qS77vP3lWJar9PH3vyeijt27s7EY86Cr74Irw8Pd09CiVSO3Uhkhwt0kJJ9HSri1gvr06HyKirJJW89poNUi0psb+XlMDBg/DUU4ltv7TU3rVVDsOqO+Saa9xNm0mT3OtfdVVs7SQRdY2Ek+jppiQPtbhTyaJF4b5gY6p9xw09WDdS+xs3wsMP24OB162Drl3tiTV9+ri30707/OlP8K9/hdcfMsS9PIWownYn0dNNSR6quFsqu3ZZd8muXdbsys+3CnfHDpg1qzo3yIUX2lSvnTrZFaNQIpVHaidBzAzW1TMT1o2iNApUcaeSU0+1uxiDzSCPx/qU42H+9OtnDx0IJTvbHglWxb598OST1mqeP9/6rSsrbaKIL76whykMHRp9v/n5Nqd3Q9uJkplqYEdFoqebkjzUx51KLr3UbnjJyLA+4YwMm/lu8uT4tL9unXv5wYPu5e+9Zx2fVfuKjbHK97nnat9tGcrUqfFpJwpUaUdPoqebkjzU4k4lWVnWd/zFFzZPSNeucNJJNv6pNiorrfLNyak95C7WML5I7Nlj9xpHShwVKs/q1fVrpw6iVdJm3AxKKSWddDxVtkktf2yM9fGmp9fc4BGpvKlSNd2+/NIuc3TqBCeeWPd0UxofqrhTzcKF1kI9dMg+r+7caXcaRIrKmDLFZvarslyPOQZuv91ds3g88cnKU1tclps8OTlWQbvJU8/4rmiV9kd8xFSmcohD+PFzARdwERch42a6NvLRRzWH/4IL7PAvWOBe3tTjiT0eOO44+1KaLs3AjmjCfPaZVXz799udEUVFdkFv1iz3+tOmWSdlsLvh669tIK4bnTvHJk8k671XL/f3Isnj84U7TX0+OO202DblOESrtD/jM6Ywhf3sp4IKiihilvMPgHE1Vy0jDf9f/hLbZVGUZKOKO5X885/u+SvfesvdUn73Xfd2vvnGfYv5jh2xyVNR4V6+bVts8mzbZg8+8PnsRhyfzz6TX3llVGLMHFfzFS3/5J+UUXM8SynlLd6iknD5Iw3/okWxXRZFSTbqKkklu3a5lwcC7vkra8v/sW+fTasWTKxaJtLCYX3kOeccm2Bixw6bszsvLyoRGrLYuAv38QwQoJhisqkpf6Thj3UY6ss339jljSOOgFNOiU+bSstAFXcq6d7dPfIjM9N9y7jfH57UqYo2bcLLIm1Jj0Qkn3h95fF6oW/fqLtvaIRId7qzjvDxzHT+hdWPMPwej6GyMtyZnZFZSWZmwx9SS0rgxhtrHtn59NPwyCPh372K4oa6SlLJ5Ze756+cMCFyvks3Ro509x0PGuReP1LQbseOiZXHhfq4RCJxOZeTTk35/fiZwITq6JLg+hGGv80x+UCo2W2o7LUhLtElDz0Ufs5ySUnsR34qLRdV3Klk8GC49Va7Ucbvt+GAkyfD97/vXv+HP7R5Q6oUb1oanHsu/PKX7vU3bXIvj2Ql795tN8kkSp4EM5jB3Mqt9KMffvx0pSuTmcz3cZc/0vDv/c4PhFrcQtGaHhRVRBi7GHDL7wVWmUfKuKsowairJN4UF9vdh1VbvceMqd2/268fjB5dXX/wYFu+d689kGDDBhtwe+WV0KMHnH++fUVDJAUdCWNgwAB44IHo/yYWeYJoqIVdTDHzmc9a1tKNboxhDHnkMYQhPED08g8ZEv5xH38hQmbDSg87Svbwr5Wr2bA2jU7dA1w5fAg9MjvEdNlr24O0f78NUQxtJ1L7xd5DzO/1MmvbLqbboYGM+fYa8kojp/qLdXoqjRMxcd7JBjC8Xz+zJHhLdUth3z5rwhUVWaXp81mXwT33QO/e0df/+c/hiSfC7/Abb4xtFevhh+15kKFkZNj+Qtvv2RP+/Ofo24+BeO5w3Mc+buVWiiiilFJ8+PDi5R7uoTe9GyzI5JcXsWf2iVBZ065JO3IdlQU5mAOtoCgbMoohPcDPbtvJ6w8fEfVlnzzZ7kUKxeuFVq3Cp8PNN9uMBGHlD+7iyZ8Npci3n1JvEb7yDLwmnXv+8xG9DxwbPm4xTk8luYwfL0uNMcOjqauukngybRocOFBt6VaFITz9dGz1//Y3d7Ms1vybkRyyeXl2k0zVljmv1yrz666Lrf0oife29GlM4wAHKMWOW1XUyNNEGOcYuemiXtB2P2Q4fov0EsgpIKvHXsyudlZpA5RkwsEcnn+4fUyX/aab3Mv79HGfDo8/HqH8uUMc8O+k1GvlDHhLKPYe5OnhP3VtP9bpqTReVHHHkyVL3KMyNm2qToIcTf1IYXZlZfZZOlqWLXMv373bJpm6+GK7he7cc+HRR21cWpxJRC6RJSxxjcvexCZKcBnnYMbVnTqwf6tuPPGoh6N/9im53/+UfpMWc+8TBzj0eX8oD90f7qFyX15Ml71/f/tAdfTRNldIv372+M6tW92nQ0FBhPKVPagsClloFtjU+itK0sLTHcQ6PZXGi/q440ltSR/crN/6JInYtMlu7du0ycaOjR9vle/06dU7RERsKrja2s/Lgx//OPb+oySRyZ98RP5cbtEj9aFLRlvuHHk6jKwuk/SdYbEmdRHpoWdeq/9lebs8KrcN5lDeJv7dZhU+3+WxNS4GvG5f8oLHhN/asU5PpfGilyuenHFG+N2RlgbDhrnn6IiU4jRSuF52ts0StGyZTaKxYYO1lB980O7HDs7Gt3ChlSUWeeJAvEL7auMMzghT3mmkMYxhYeGArkRhdbvR/eTNuIUJprXfF9MwTzv4L2bdMIbKeaNhT3vM58ez8JYL8Q3/0rWd7NZlrv36Ou3B560Z/ZJW4WPY9nNIr8wI6zfW6ak0XlRxx5OxY2HgQKt4MzLsq2vXyL7j7dvdy9PTrQ86GJ/PRpeEbqgpK7Onv7qxe3d1suVo5GkgyUqxOpaxDGQgfvxkOP+60pXriOFzjZsZswLPXDUMtzDBrEDrmIb57Rc6VvvJqyjKZveyThw10IS1Uxgoc+03sK0j/XeOxF+eRUYgh4xADl0PDeC6Jc+79hvr9FQaL+oqaQhlZVY5tm5tc2b6fPYAgbVrYflym5xp6NDIKeXy893Li4ps7uqvvrJJm3r3hrPOijrXRw2uuMLKGY089WDmOCijjN3spjWtyaLulK0FFLCWtfSkJ+1od7g82nZ8+LiDO1hR/C1L8nfSv0srTsodiDjKrazYw+78LFp3KSErt9qVUFBcwdr8Unp28dEu1zE9q5R3FN86+Zvc7ZyiAg+//rXdhfnxxzaic9So6mEu8xSzOyuf1iVdyCrPpfKrIe4d7G3HFU9spmx9zxqX65JLIuyxr/Rw5TszMb2/ZWPrr+hU2Jejdo+oHodapueyZXZaxXk6uPZbV7kSO3UqbhHJAD4C/E79140xdyVasEaNMdXp4kRscqbRo+3huW+/HV5+9dXuOwnbtrUJmULx+aw5dMopNcP/2rSJbKVH4pNPbDKoaOSJEYPhTSf7niBUUMFoRnM1V+N1mVqVVHIf9/EN3xwu60Y37ud+5jAn+naM4b77K/nmgeutn7fcS7ef/B/3/6WAOX8azKwHBiJiqCj3MPonG7nyL5/z0J8q+OaB82rUf+CvBWT40qL+vJEul9cLv/udzcgLMG8evPAC/PVvhg+G38+sgQ8gRqjwlDN640+g642wJje8ofRSFr/ZhXfeqnm58AUg4O7L6JiTTeaBofQ+UO12i9f0jJVU9dsSqTOOW0QEyDbGFIiID1gI/MoYszjS3zT7OO65c+2dGbzBJT3dbl5Zsya8/Mwz4Sc/CW9n4UJ45pma9f1+G+UxYUJ4/d//Hlatil5Or9c6MaOVJwZmjoO5zOUFXjgclgeQTjpnciY/Ibz9J3mS+cwPK29HOwooiL6d54uZ/6uLoTDIEs0sot2IVRQsOobSwmptkJ5ZTocRa9i6qFdY/T7XvcdDj5ZXf6A6iHS5srJsjHQoGa2LMLs6UuotrJanPBOunULZC5dT0/1hkF6bSN/RO+xyteq5lz3r2oTVbzt0E0/f3jus33hNz1hJVb/NhbjGcRtLVWyRz3nFf9dOU2LWrPBdiWVlNt2bW/mHH7qnTB0xwibMyM62M9nvh7PPhksuce937drY5Cwvj02eKKnScbOYVUPZgnV3fMiHVBDe/kIWura3hz2xtfPgyJpKGKA4iz0fDq2htAHKir1s/fAo1/rfPnMWZeXRZ1CMdLnclDZAyf5MSktqtl/mLaZswUm4+axNfk/Xy1WwuS29zlgHUom99Qytj8nnL7/t4dpvvKZnrKSq35ZIVA8qIpIGLAWOAJ40xnzqUuda4FqAnu3bx1PGxseBA7HVr1Kgbo69s8+2psehQ1Yj1BazFa9ZXps8dRBsmB7AfRzKKaeU0jA/tZsSrlXMSO181879D0wEZ22k8oCPA4Xl7Mpbx8Yj/kLngn4c+90PSDNejIGVK+0RX507w7HH2oeXH5xt6DZhAavNGnp6unH8vjN5661abqONvWHIyppl33WKSc7ycrjniiORn5azbtd+erZtRV4Mhex+AAAX3ElEQVRGr4hd1md6lpTYCNPQzxsL8bwtlNqJSnEbYyqAoSLSGpglIkOMMctC6kwBpoB1lcRd0sbEEUe4b27xet03z+TmuqdFrSItza7Y1EV2NhQW1l2vLnlat65dnig5giNYRvg4tKa1axrVLLIoIvosSrnkuraTfcIKCueeSNgDo78UStNdysug1BdWLp138mju3WxlCxXHGLyV6eSUtueOOR/ztzu6sHmz/a70em2Qzx33F/G3i85gc+5yKjyBw/U9nvWuaWABOGpleNkJS2DuaRHkD09wVXW5RLwc3bVuoyjW6ZmXZw9R2rKl5uf94x+tXz9aYu03TtOwRRJTOKAxZj8wDzg7IdI0FSZOtM/Jwcvx6elw5JHu9XNz47N0f/XV7uWjRrnLM2GCe/lVV8Usj1t89kQm4sd/OIoBrG/6Kq6qUVbFAAa4tu3HPW49jzzXdq7+0yrILgYJsuCzCul2399dy/33/QGyi8LLn7iFfLG7LQNppRT7DrEnK58//mchGzdaK7RqW/iePXDvS/lsbP0lJb6CGvU73PKiq/xtzloCLlZruz896Crn6GvW4PdLgy9XpOkZaTr07Vu9ezL48/7979H3WZ9+6zENFYdoFic7AAFjzH4RyQTeBx4yxrwT6W+a/eIk2FC+mTPtJpjOne328Ucftaedh5KWBi+9FJ9dDp9+Cv/4h30uzcyEcePsYqabPIMGRS6PkrrW7PLJZyYz2cAGOtOZi7mYQbi3fw3XcBCX8YlAGmm8xEuum2o+XVbIP+7uyYElR5B55FbG3bmKN0fdyMFlPeDuu2HJ8XDkOrjzjzBqASwb7F7uRoedsNvlRANvAA7mQmbN/eFplV4u/WkhM6alEwjYXYhnnQWLZnXkYEb4MTtppHHDsqd5+e6+Vv6euxj3Yy/ndju2oZfrMLFMh3hO2wRNwxZBLIuT0SjuY4CXsLaDB5hhjPlDbX/TIhS3G1dd5e7KSEuzy+0Z4bvZGjvx3FRzFVdRuDUPnvgVLDoFBi+Hmx6Do1a71k8jjUe2vsbcJ4awelE7ug8+yHk3raXbUYcit08MriSArV3D5TllEexz8RF4A9bFcfOjsOoo6LYVHrmZtLM/5NF/r+D/+j7L6naL6H5wMOetvYk7Tj+FQn/4ymUaabzAC2QQPh/2vn0Ks498okY73Q4dFdtnipHapu0jj9hokdWr7YlB551n08HGyt69MHt2w9tpzsRVcdeHFqu4n3nGBvEGLyKKWBfKvfemTKz6kIhdkI+tfYdPTnwcirKgzA9pAfCX4Z9zEaWjPgir32btiZSeuICyojTKy9LwpFXi81dy25wFDBq1O6z+MzzDPObVWAQVhEwyXX3r6WsHU3bigjB5MoatpOTj8Psnvetuyra3dRYRhargqrYPPkzJTQ9SllZEeVoZnso0fJV+Bu88g686vUdFWqCGPEdyJPcSPh+2s53buI0yyiin3Lbj8XIbtzFoZuK2TkSatr162dj0sjLro/Z47Nr5bbfFZi1v327/pqHtNHc0rWuqmDDBHoxbZVn7/XZBcfLk1MrVSCj+7T3W1VDm+LQrfFCUTdm1f3Otv++3t1F80Et5mXUUV1Z4KC3yMuXa413rT2AC7Wh32JL14yebbFc/OUDZb+91lady6TDX+oEd7YKUNs7/wt47f02x9yDlaTYdQaWnglJvEVtbraRdcfcweSbjPh+mMpViiimnvLodSpnClHrnV4mGSNM2J8f6u6sWFisrbRTIlCmxtT91anzaUarRfUvxJDcXHnsMFi+G9ettIogRI5pUvFMi842sntsNKsNX68z63nCwFeSGuEDmno6pDLctvlufQ9FBb43t7GCjUB7jMRazmPWspytdGcEIruRKd4Hmnu4qT1mJexyciRQ5EvBitnWG7jW3Ve7O3sTTb2/j687vs77NEroe6s+I/MvJKg/ZNeko5WUsw7hskfiO7yiiiKxxMxNygSJN28mT3dPCf/edzcoQ7bRetiw+7SjVqOKONz6fPSx35Mi66zYCQvXAUpbyMi+zgx20pjXjGMcYxkS0WmMhKzdA8UGXOPW0SshwSQidexAOupyr5TGkZ7jHhH/N17zO64fl9+BBEFeFGLH9+tA6PE+6x6SRHchjZP7ljMyvO2VrFlkUUxzeDp7osh42ALdpm5VlLeUweTyxLVjGqx2lGnWVtGBClfaXfMljPMZ2tmMw7GMfL/ESs5kdl/6O/dU8yApZBfMXk3nJ25AeCKvvGbICt3Smno478aaHK+JI8nelq6s82b96PiyDblUSRjciKZn0vGL8IeuMvgo/J2++BK+JQjM5F+IczgkLjfTh42ROtjlbkpV+0eGcc8IzDPt8cPLJseUYiVc7SjWquFsobjpgOtMpo2ba2FJKeZ3XXU+ciZUNN/0PTHgV/CWQtx8yi+C0+fDU9XShS426OeRQOX8krulMt3aixGVHRyT597OfznQOa//Rm3ocPm8iK8sq5kGDIqdDD82oW0VaWSanbLwMX4WfrLI80sszGbTrNK75PIaj5maO47yZL3Iqp+LDRxZZpJPOIAZxDdckXWmDjfxwG59rrklNO0o1GlXSgqjr3r+CK1yP/vLi5VmeJZsI6UUjsJ/9bGELHZ1/h9vf3hmWD4Y+30K/DYfbX896FrGIwQxmJCMZL2MJV9wAhj9sfIWjetXcdleX/Bt2FPHxin0M+u50Rnmrsy7u2webN0PHjjbG+IorYjvKy+uFZ5+Fsnbb2Zy7nI6Ffehc2C/6BkLYlxGfduJF6PjUl40bbZbifv1s6lulJrFEleiDSgshGoOtM53ZyMawcj9+163nkaikkmd5lo/4CB8+AgQYwhA60pF88qHLDvsKav8VXmEBC/DhYwELbEKqzB9CscvKlRh6dwv3lUeSP70ik6nXj+Cjl/rgy6hgQVE6Hw+xh/ZmZNhsuW3aBLXT2SqZUDwe9zMb/X67Fyq7pAttSrqEV4iRNnFqJ16Ejk+sVFbaL7aPPrJWdyAAQ4LGX4kddZW0AKJ9yr6My8IWwfz4GcvYmM5yfJd3WcACAgQooogAAZaxjDzyXNs/iqNYyMKw+hm3P4brkV0XzCHDxTkaSf5Bjz/DghePIFCaRtEBu7tx2TJ43v2gGC67LNyf7ffbo7/cyseO1TMba+Pdd2HBAquwi4qoc/yVutHp1syJxTU6lKHcyI10pjOC0IY2TGIS53JuTH3OZnaYrzlAgFWs4pf8Mqz9b/nWtX7JHXfA7feBzzlz0VMBl75KxZsXHo51jkb+b5+4IMw/HQjAokXuyY+GDoUbb7SWt4i1NidNgp/9zL383NiGp8Uxe3b4+kBt46/UjbpKmgmlAQ9vfNqNpRva0L9LARl/zg+Lcw4mQIBP+ZQNbKALXTiVU8kii+HOP4OpdwhgpAyAlVRyFEcxnvGH46xP5VSmMc1m9XvjYlh6HPRfCxOm27ju++60r0oOmxlCGmWUuZ6O4yb/tAPukR2VlVahuEU2DBpWyrFZS1izMUC3Lh6OHzwUkVyGD4fhw21cckMSJAUCNu3Mhg3QpYtdvGuu8cxFERJC1jb+Su3okDUD9hxK58Tbx7Ct0E9JgQ9/djm+N4dw78dz6TqgIKz+IQ5xO7dzgAOUUIIfP9OZzr3cezh0riFx2wMYwJd8GVbehjbcwR1h/fbacxyrTnwJdnaEglaQXQD/7z58H59O+YDlNgY76NmwIx3rPNvysPwzxzG4PyxdGr4JpGNHd2W5vWQvv77rEBW7joGCVnybXcBCfxm3PbiBYa372vYboLQPHYLbb7d5wkpKrLtl+nSbFaGre+Rik2bw4NjGX6kbdZU0A2579Wg2786kpMAu2JUWeinc6+Opq09wrf8qr7Kb3YcjMEoppZBCniKG8LVaiLSQWUyxa79bbvsFbO5ulTZAYQ7sbUO7q98ik8zDlrUHD378XGvP64iOcTOZ9OocMjOrLTuPxyrLayM089C7y6jY0rmmPPvyeOzZ8C/B+vDqq/bQ3KrIldJSm+TpqfgMf6Nj0iRiGn+lbtTibga8sbgbgYqa27SN8bD+s7aUFKaRkV1zl+FiFoedRmMwrGc9JZS4Zq2Lha/52rXcLXOfwVDwxg8gEBI8bdLY9VlvHil8gg+z32INa+hOd87jPLrTPSZ5uhxZwCOr3+HdyeexZk11drruEZrZ9v7RrvKUfDGQA4FC8nyxhUWGsnhx+GFGxtjt5iUlzS/SoksXm2Xw3XeJavyVulHF3UQJXnQs/4UhUjZTjyc8Tj/NLbt/Vf04PITV1r77H0T2xbf3tOEKrmigRNCuezFXVDVTR84PSauIeKhqWjzGp5bhaa7RKe3aUT3+SoNpptOkZXHspK/tbsRg0gJ0P2Mt6ZnhgcejGIWPmnHQHjwMZnBccmL0w33TSCaZrv22nzQHn7+mCepJq2TwGTtd5W8woUo7JPNe7/OWuY5nzqlfkeNr+Flbo0aFHy3q8VhfsObuUKJBLe4mSKje2fOHG2DhX2HFIAj4IL0M2u7lwD8uxvD7sIXG8YxnFavYwhbKKceLlxxyuJ7r4yLfatwPRiimmL70ZRvbavR75x/K+evC/WxZkUt5wIM3vZKctmVc/4//xkWew9SWGrXqvZnjuP3ME7jh63WUrukNAS+kB/C0PsD/uzI+mf/Hj4dVq+wZj+Xl1Wc8Xh+f4VdaALrlPZlUVtozonJy6h0D5faEfwVXUGJKYN5p8PUx0HcDnDOHNC88z/NkkMFBDpJDzuGFPoNhOcvJJ5+OdGQYw2J3cURgPOMjvvckT/Id34X1awwsn9eB/K/z6Ni3kGHn7CDNm7ozpyuN4Z15BSz/Oo1efSsYW3QF6Z742TnGwPLl9kivjh1h2LDYT1VXmhe65b0x8t578NprNnDV47Ep0y69NC5OzTzyKJESOH2efTmkkc585jODGZRRhgcP53AOl3IpHjwMcf7FmzTSwhY/q2hFKzrQIaxfERhy+i6GnB5+RmMq+EDeZ9bpr1F2ehkr8ABvHx63eCR8ErHbvofEf/iVFoD6uJPBokX2GJDCQrvzorQU5syBGTPi0vyFXBiWDrQqs9yrvEohhQQIUEopc5jDDOLTbyT609+1PI+8BkesJINFLGIqUyOP27iZCT2RRlHqQhV3MpgxwyrrYEpL7V7g0LgwF2aOq365cQZn8CN+hB8/GWTgw8cIRrCTnZRSs99SSpnN7IgWcTzYR/gBuWB93InsN17MYEZ04xasvFWRK0lEXSXJYO9e9/Iq67uW7WPRPJULwljGcj7ns5vdtKENWWRFDKOrsiLr2n1YX/YTfhoMQAUVCe23ik++LuTrLz307lfBmadk46ljm+MmNrGRjXSiEwMYwF7cr5fruKnCVlKAKu5k0KsXrHaJtGjVym4pi0CsrlQ/frpRHfnQi16uER6taBVTmtZYSVW/RaUV/PLCHhz6aCh4bBjhy3228Of/fE6XduFxdgECPMzDrGTl4fj1jnSkO91Zx7rY5U/QmZCKEoq6SpLBxInhAbrp6ba8IUkv6uqWiWFx2emkM5GJcTlDsrH1+8C9aRyaNwyKsu129YJWBFb15a5r3MP43uANVrCCMsoocf5tZStevPWXXy1wJQmo4k4GAwbA739vd1jk5ECfPjaL/KhREf8kHobbAAbwe37PYAaTQw596MNN3MQoIvcbD1LV75rnRkFJiBsm4Gf/u6dQUBK+O3MucwlQ86zLCipYy1pu5/aky68o0aKukmTRvz/cdVfyu6U/d9Ey+jUlEQ6LNOKa9zk0B3gwfelbf/nVZaIkGLW4Gxm1RY8otdPhvE/BG3pafCXpx6ymdU64jXI8x7vmZulJz4aHLWrIoJJAVHErzYabH9qJdNgDWU7GrYxiyD3E9c8vda1/OZeTS+5hf7YPH5lkMpnJyRJZUeqFukoaEWppN4y+XTN4atVcXny5kvWfdKTLoH385JoAPTu5hx+2pS2P8zjzmc8a1tCDHpzBGbSmdfyEUreJkgDqzFUiIj2Al4HO2AOkphhjnqjtbzRXSewk6t5ewQqmMpUtbKEd7RjPeE7m5MR0pkRGlbdSB/HOVVIO3GyM+VxEWgFLReQDY8yKBkmpHCaRSvt+7j+8CLeVrTzJkxRRxBjGJKZTRVESTp0+bmPMdmPM587Ph4CVQHzyWyoJZRrTwiInyihjOtOpJAF5rpXI6EKlEkdi8nGLSG9gGPBpIoRpSSTjyXkzm13LiyiihJKEbz1XQlB/txInoo4qEZEc4A3gRmPMQZf3rxWRJSKyZNfBsLeVIJJ173agg2u5D1+TyNKnKIo7USluEfFhlfY0Y8ybbnWMMVOMMcONMcM75ObGU0alnoxnfNjWbT9+zuf8uJwtqdQDje9W4kCdrhIREeB5YKUx5tHEi9R8SfZT8kmcRDHFTGMaBRSQTjoXcAEXcVFyBWmO1HFuZZ2o20RpANH4uE8FJgHfiMiXTtntxpjZiROr+ZGqe/Q0TmM0oymhBD9+tbTjgdvFnDlOLWkladSpuI0xCyGBKd2UhCNIQtOptiji+Q2sVrdST3TnZALRe7KZEO2FrKoXi+WtylupB/rcrCi1UR+lqopYSTCquBOE3rvNhGT4rdU3rsSIKu4EoEq7mVEfxVrXCc9ufagCV6JEFXecUaXdTGmIYtVJocQZVdxxQg9AaCGo8lYaAaq444Dek0pU6ERR4oQq7gai92ILpCG+aH00U+KAKu4GoPdfC6ahC4k6eZQGoIq7nuh9p6jyVlKFKm5FSSWqvJV6oFveY0DvMSWMKqu7IZNDE1QpMaIWd5So0lZqJR5uE51kSpSo4lYURWliqOKOAjWElKjQbetKklDFXQeqtBVFaWyo4laUeKNWt5JgNKrEBbWylQYTj2gTRYmAWtwh6H2mKEpjRxW3oiQSdZsoCUAVt4OG0SoJQ5W3EmdavOJWha0kBVXeShxp0YpbFbaiKE2RFqu4VWkrSUetbiVOtEjFrUpbSRm6u1KJAy1OcavSVhoFqryVBtBiFLcuQiqNDlXeSj1pEYpbFbaiKM2JOhW3iPxDRHaKyLJkCKQoLQq1upV6EI3F/SJwdoLlSAjqHlGaBKq8lRipU3EbYz4C9iZBlriiCltRlOZKi/BxK4qiNCfiltZVRK4FrnV+LZDx41fHq+0E0h7YnWohkoh+3uaNft6mTa9oK4oxpu5KIr2Bd4wxQ+ovU+NDRJYYY4anWo5koZ+3eaOft+WgrhJFUZQmRjThgNOBT4ABIrJFRH6aeLEURVGUSNTp4zbGTEiGICliSqoFSDL6eZs3+nlbCFH5uBVFUZTGg/q4FUVRmhgtVnGLSJqIfCEi76RalkQjIhtF5BsR+VJElqRankQjIq1F5HURWSUiK0Xk5FTLlChEZIBzXateB0XkxlTLlUhE5CYRWS4iy0RkuohkpFqmZNNiXSUi8mtgOJBrjDkv1fIkEhHZCAw3xjSnmNeIiMhLwAJjzHMikg5kGWP2p1quRCMiacBW4CRjzKZUy5MIRKQbsBAYZIwpFpEZwGxjzIuplSy5tEiLW0S6Az8Enku1LEp8EZFcYBTwPIAxpqwlKG2HMcD65qq0g/ACmSLiBbKAbSmWJ+m0SMUNPA78FqhMtSBJwgDvi8hSZ4drc6YvsAt4wXGFPSci2akWKklcCkxPtRCJxBizFfgzkA9sBw4YY95PrVTJp8UpbhE5D9hpjFmaalmSyKnGmOOAc4BfiMioVAuUQLzAccDfjTHDgELg1tSKlHgcl9AFQLNONSgibYAfAX2ArkC2iExMrVTJp8UpbuBU4ALH7/sacIaITE2tSInFGLPN+X8nMAs4MbUSJZQtwBZjzKfO769jFXlz5xzgc2PMd6kWJMF8H/jWGLPLGBMA3gROSbFMSafFKW5jzG3GmO7GmN7YR8u5xphm+40tItki0qrqZ+AsoNkeimGM2QFsFpEBTtEYYEUKRUoWE2jmbhKHfOB7IpIlIoK9vitTLFPSiVt2QKXR0gmYZec4XuBVY8y/UytSwvklMM1xH2wArkqxPAlFRLKAM4Gfp1qWRGOM+VREXgc+B8qBL2iBOyhbbDigoihKU6XFuUoURVGaOqq4FUVRmhiquBVFUZoYqrgVRVGaGKq4FUVRmhiquBVFUZoYqrgVRVGaGKq4FUVRmhj/H7mhLosyjZE+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a150db350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 20# how many nearest neighbors are consulted\n",
    "\n",
    "X = iris.data[:, [0,1]]  # we only take the first two features. We could\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='distance')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3))   # this makes both axis equal \n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'distance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting figure shows the predictions of kNN when $k=1$. If `x_new` is in the blue region, the prediction will be the blue class. From this picture, we can observe a small blue blobs inside the predominantly gray area. This is because the nearest neighbor in this area is the blue example. \n",
    "\n",
    "**Question 1**. Change value of k to 3 and observe if there is any difference. Discuss what you see and why.\n",
    "- The changes we see now is that before when k was 1 for outlier points like a blue point in predominatly green area  there was a small region of blue( basically an island) when the surronding areas was majorily green and red. Now when we changed it to k =3 we no longer see those small pockets as much and the colors seem to be clear cut when their are outliers. This is now occuring as now it is looking at the 3 closest neighbors which means for a single point in a predominantly one color area, it will see that the majority is the one color instead of the single point's color. Example the blue point right bewteen red and green area had island of blue when k=1 but when k=3 it is now part of green area.\n",
    "\n",
    "**Question 2**. Change k to an even higher value, let us say to 25. What do we see now? Discuss.\n",
    "- What we see now is that there seems to be a clear border or boundary of the predictive colors as before there were pockets of blue and green as they are so mixed that they were fighting bewteen each other. Now with K=25 it begans to look at points a little farther away and decides a majority so even though there are green and blue mixed in the green region, since out of 25 points there are closer green points than blue it becomes green.\n",
    "\n",
    "**Queston 3**. In the line that creates `clf` change weights='uniform' to weights='distance'. Check the documentation or google to understand what it means. Explain. Run the code and discuss if you see any difference.\n",
    "- What the line of code does is that when weights = uniform from the K values we get all are deemed as equal even if one is farther than other and determines majority based on them being equal. When it changes to weights=distance this means that a closer a point to the value the higher it weight than points farther away. I think the best way to explain is through an example so lets say we have a new point we are trying to quess type and lets say there are only 3 other points; a blue point really close and two green points really far from point. With weight = uniform and if k=3 then it will assume new point is green because even though green points are far away they are a majority of the points in queue, but when weights=distance it will say that the one point really close will weight much more greater than far away points and will probally predict new point is blue.\n",
    "\n",
    "- When we do distance we began to see alot more pockets for k=3 and when k=25 we still see clear boundarys like we did before yet we see the shape of color when it was uniform changed. \n",
    "\n",
    "**Question 4**. Take a look at the code and try to understand what each line of the code does. Explain each line of code.\n",
    "\n",
    "```python\n",
    "k = 20# how many nearest neighbors are consulted\n",
    "\n",
    "X = iris.data[:, [0,1]]  # we only take the first two features. We could\n",
    "y = iris.target # The actual type of flower\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color map of background colors (light shade of red, green , blue)\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "#Creates color map of color of data points (dark shade of red, green , blue)\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "#In this case gives weighting of distance( closer distance higher weight)\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='distance')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Finds the min and max of x-values in the grid  \n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "# Finds the min and max of y-values in the grid  \n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "# Creates a matrix with min and max x-values and y-values as the boundaries \n",
    "# Uses 0.2 as the difference between each point in the grid\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# The xx, yy values changed into a continuous array. Then they are translated\n",
    "# as slice ojbects and concatenated along the second axis and are stored in Z\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "#The contious array Z is shaped into dimensions of xx\n",
    "Z = Z.reshape(xx.shape)\n",
    "#Creates a figure so we can plot\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3))   # this makes both axis equal \n",
    "#Plots the colormesh backround using light shade of colors\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "#Sets the limits of the plot to be min and max values for x and y axis\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#plot title\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'uniform'))\n",
    "#Show Plot figure\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "  As mentioned above, the typical mechanism for testing accuracy of a `predictor` is to split the data randomly into training and testing, train `predictor` on training data and test its performance on test data. Let us see how it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "print X_train.shape\n",
    "print X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. What is the size of the resulting objects?\n",
    "\n",
    "**Answer 5** \n",
    "- The size of the training data is now 100 flowers and the size of the test data is 50 flowers. This is beacuse we wanted our test size to be 33% of original data so in our case 150 was total and a third would be 50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created training and test sets, we can train a kNN classifier using the training data. Before moving forward, let us take a second and take a look at the documentation for kNN implementation in sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Let us train the kNN predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 1   # number of nearest neighbors\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "predictor.fit(X_train, y_train);\n",
    "print predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained `predictor` we can use it to provide predictions on any example `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3 3.4]\n",
      " [7.3 2.9]\n",
      " [6.7 3.1]\n",
      " [6.5 2.8]]\n",
      "[[2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 1]]\n"
     ]
    }
   ],
   "source": [
    "# select the first 4 test examples\n",
    "i = [0,1,2,3]\n",
    "x = X_test[i,:]\n",
    "print x\n",
    "# predict its label\n",
    "yhat = predictor.predict(x)\n",
    "# compare predicted and true labels\n",
    "print np.array(zip(yhat,y_test[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**. Did your kNN predictor do a good job in predicting labels of the first 4 test examples? \n",
    "\n",
    "**ANSWER 6**\n",
    "- It did a pretty good job of predicting the labels as it was able to guess 3 out of the 4 correctly which would be a 75% accurracy. Overall we do not know if it is truely good as we need more points to try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**. Write a piece of code that calculates the accuracy on those 4 test examples (number of correct guesses divided by the total number of guesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "def accuracy(yhat,ytest):\n",
    "    correct=0.0\n",
    "    guess=0\n",
    "    for value in yhat:\n",
    "        if(value==ytest[guess]):\n",
    "            correct+=1\n",
    "        guess+=1\n",
    "    return (correct/guess)\n",
    "print str(accuracy(yhat,y_test))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**. Find the predictions on all test examples in `X_test` and calculate the accuracy using your code from *Question 7*.\n",
    "\n",
    "Pay attention that methods in sklearn.neighbors.KNeighborsClassifier allow you to test the accuracy in a faster way (you should not use it to answer Questions 7 and 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [0 0]]\n",
      "The accuracy of all test values is 0.74\n"
     ]
    }
   ],
   "source": [
    "###ANSWER 8\n",
    "yguess=predictor.predict(X_test)\n",
    "print np.array(zip(yguess,y_test))\n",
    "print \"The accuracy of all test values is \"+str(accuracy(yguess,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictor.score(X_test,y_test)\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**. Train `predictor` using different choices of k. Try $k = 1, 3, 5, 15, 25, 50$. Report the accuracies on the test data (you can use the score method). Which choice of $k$ resulted in the highest accuracy? Comment briefly if the results make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for when K is 1 is 0.74\n",
      "The accuracy for when K is 3 is 0.8\n",
      "The accuracy for when K is 5 is 0.82\n",
      "The accuracy for when K is 15 is 0.78\n",
      "The accuracy for when K is 25 is 0.82\n",
      "The accuracy for when K is 50 is 0.7\n"
     ]
    }
   ],
   "source": [
    "k=[1,3,5,15,25,50]\n",
    "for value in k:\n",
    "    predictor = neighbors.KNeighborsClassifier(n_neighbors = value)\n",
    "    predictor.fit(X_train, y_train);\n",
    "    accuracy = predictor.score(X_test,y_test)\n",
    "    print \"The accuracy for when K is \" +str(value)+ \" is \" + str(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9**\n",
    "- When k was 5 and 25 the accuracy was the highest of about 82%. This makes sense as we want more than one point to determine what should be our guess so 1 and even 3 is not sufficent. I think using 25 as k was just chance in producing also the highest accuracy as with too many points, it will no longer be even looking at things really close to points but even points pretty far from guessing point. Overall, I think 5 is the true greates choice for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**. Other than choice of $k$, `KNeighborsClassifier` allows you to make some other choices. For example, in *Question 3* you saw that you can use a weighted prediction. There are few other options. Study the documentation and summarize in few sentences what other options you have when training the kNN classifier.\n",
    "\n",
    "**Answer 10**\n",
    "Some of the other ways we can train the classifier is by\n",
    "- n_neighbors which is what we have been using(look at n neighbors to determine classification)\n",
    "- weighted has 2 options of uniform and weighted in which uniform just gives each point close to guessing point same weight while weighted says that points closer in distance to guessing point will be weighted more( greater status) when deciding final guess\n",
    "- different formulas in calculating distances like euclidean, cosine, jaccard, etc.\n",
    "- different algorithmns in deciding what is considered closest neighbor ball tree, brute, kd_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 7.B Training kNN classifier on Iris and Newsgroups data\n",
    "In this part of the lab you will use your knowledge to train and test accuracy of kNN classifiers on Iris and Newsgroups data.\n",
    "\n",
    "#### Iris Questions\n",
    "In Lab 7.A you used only the first two attributes of Iris for prediction. You have 2 questions:\n",
    "\n",
    "**Question Iris 1**. Train kNN classifier on other pair of attributes. Use $k$ of your choice and feel free to keep other choices at their default values. Which pair of attributes results in the highest accuracy?\n",
    "\n",
    "**ANSWER 9** \n",
    "- After running the following code I can see that using the first and fourth attribute produced the highest accuracy of 96% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for when attributes are 0 and 1 is 0.66\n",
      "The accuracy for when attributes are 0 and 2 is 0.9\n",
      "The accuracy for when attributes are 0 and 3 is 0.96\n",
      "The accuracy for when attributes are 1 and 2 is 0.88\n",
      "The accuracy for when attributes are 1 and 3 is 0.9\n",
      "The accuracy for when attributes are 2 and 3 is 0.94\n"
     ]
    }
   ],
   "source": [
    "y = iris.target\n",
    "X = iris.data #load all data and split it\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 7)\n",
    "firstA=[0,1,2,3]\n",
    "#Using 7 as k will calculate accuracy of all pairs (0,1),(0,2),(0,3),(1,2),(1,3),(2,3)\n",
    "for value in firstA:\n",
    "    hold=3-value\n",
    "    if(hold>0):\n",
    "        j=1\n",
    "        while(j<=hold):\n",
    "            X_train2=X_train[:,[value,value+j]]\n",
    "            X_test2=X_test[:,[value,value+j]]\n",
    "            predictor.fit(X_train2, y_train);\n",
    "            accuracy = predictor.score(X_test2,y_test)\n",
    "            print \"The accuracy for when attributes are \" +str(value)+ \" and \" +str(value+j)+\" is \" + str(accuracy)\n",
    "            j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Iris 2**. Train kNN classifier using all 4 attributes. Report the accuracy on test data set. Play with parameters of kNN to try to find a combination that results in the highest accuracy. Can you find something that works better than $k=3$ and default choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using default, all atrributes and k=3 is 0.94\n",
      "The accuracy using weight=distance, k=15, and metric=minkowski is 0.98\n"
     ]
    }
   ],
   "source": [
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print \"The accuracy using default, all atrributes and k=3 is \" +str(accuracy)\n",
    "\n",
    "#AFTER TRYING DIFFERENT PARAMETERS\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 15,weights='distance',algorithm='auto',metric='minkowski')\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print \"The accuracy using weight=distance, k=15, and metric=minkowski is \" +str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newsgroups Questions\n",
    "In Lab you already got experience working with the newsgroups data. \n",
    "\n",
    "**Question News 1**. Since kNN is a relatively slow algorithms, create your data set by picking 1,000 examples randomly. Remember to record both attribute values and labels. Split your data into 66% training and 33% test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordlist = np.loadtxt('wordlist.csv', delimiter=\",\",dtype='str')\n",
    "documents = np.loadtxt('documents.csv', delimiter=\",\",dtype='int')\n",
    "documents = np.transpose(documents)\n",
    "newsgroup = np.loadtxt('newsgroup.csv', delimiter=\",\",dtype='int')\n",
    "groupnames = np.loadtxt('groupnames.csv', delimiter=\",\",dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the X_train data is 667\n",
      "The size of the y_train data is 667\n",
      "The size of the X_test data is 333\n",
      "The size of the y_test data is 333\n"
     ]
    }
   ],
   "source": [
    "##ANSWER 1\n",
    "#Gets 1000 random ints bewteen range of number of documents\n",
    "rand1000 = np.random.randint(documents.shape[0], size=1000)\n",
    "X=documents[rand1000] \n",
    "y=newsgroup[rand1000]\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.333)\n",
    "print 'The size of the X_train data is ' + str(len(X_train))\n",
    "print 'The size of the y_train data is ' + str(len(y_train))\n",
    "print 'The size of the X_test data is ' + str(len(X_test))\n",
    "print 'The size of the y_test data is ' + str(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question News 2**. Train a kNN classifier ($k=3$ and defaults) on the training data and test its accuracy on test data. Record the computational time needed to run this. Report the accuracy (pay attention there are 4 types of documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using default, all atrributes and k=3 is 0.5825825825825826\n",
      "The Time it took to calculate is 0.0476222038269 seconds\n"
     ]
    }
   ],
   "source": [
    "#ANSWER 2\n",
    "import time\n",
    "start=time.time()\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 3)\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "end=time.time()\n",
    "print \"The accuracy using default, all atrributes and k=3 is \" +str(accuracy)\n",
    "print \"The Time it took to calculate is \" +str(end-start)+ \" seconds\"\n",
    "#yguess=predictor.predict(X_test)\n",
    "#print np.array(zip(yguess,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question News 3**. Train a kNN classifier using different values of $k$, hoping to improve accuracy. Try to use some other distance other than Euclidean. For example, cosine distance is know to work better than Eucliedean on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using cosine,brute, and k=27 is 0.6816816816816816\n"
     ]
    }
   ],
   "source": [
    "#ANSWER 3\n",
    "#AFTER TESTING DIFFERENT PARAMETERS THIS RESULTED THE BEST\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 25,metric = 'cosine', algorithm = 'brute')\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print \"The accuracy using cosine,brute, and k=27 is \" +str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Nesw 4**. Transform the original 100 attributes into 5 attributes using SVD. Train and test the kNN on the transformed data. Report the accuracies. You might be surprised to see that accuracy could go up. Discuss why this might be so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n",
      "The accuracy using cosine and k=29 is 0.7177177177177178\n"
     ]
    }
   ],
   "source": [
    "#ANSWER 4\n",
    "U,s,V = np.linalg.svd(X,full_matrices=0)\n",
    "reduced5 = np.dot(U[:,0:5],np.diag(s)[0:5,0:5])\n",
    "#reduced5= np.dot(X,V[:,0:5])\n",
    "print reduced5.shape\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced5, y, test_size=0.333)\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = 29,metric = 'cosine', algorithm = 'auto')\n",
    "predictor.fit(X_train, y_train);\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print \"The accuracy using cosine and k=29 is \" +str(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER 4**\n",
    "\n",
    "- The reason why the SVD produces a better accuracy is because even though now it is looking at a reduction of 5 attributes, before looking at 100 attributes meant calucalting distances in a high dimension space which has been known to all be realltivly same distance thus producing error. Now with a reduction of the dimensions to 5 the distances are more meaningful and therfore the predictor is better at guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
